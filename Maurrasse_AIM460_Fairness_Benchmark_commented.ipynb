{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16f8b9f",
   "metadata": {
    "id": "c16f8b9f"
   },
   "source": [
    "# **AIM460 Capstone â€” Cross-Domain Algorithmic Fairness Benchmark**\n",
    "**Runtime:** Google Colab (GPU recommended)\n",
    "\n",
    "**Methods:** DSAP (pre), Equalized Odds via Exponentiated Gradient (in), Threshold Optimizer (post), FRAME/SLSD stubs, DeAR (vision).  \n",
    "**Datasets:** ACSIncome, ACSEmployment, COMPAS, CivilComments (manual upload), FairFace (optional demo).\n",
    "**Metrics:** Accuracy, AUC, EO gap (TPR), Eq-Odds gaps (TPR/TNR).\n",
    "**Exports:** figures/CSVs in `/content/exports/` + release + Overleaf pack."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ðŸš« Disable W&B globally (prevents login prompt)\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"DISABLE_WANDB\"] = \"true\"\n",
    "print(\"âœ… Global W&B disabled\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZccmFvih-oN",
    "outputId": "8a05f990-b471-4afd-9959-8285b5e8aac9"
   },
   "id": "VZccmFvih-oN",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Global W&B disabled\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ðŸ§¹ Remove wandb completely so nothing can call it\n",
    "!pip -q uninstall -y wandb pathtools setproctitle docker-pycreds GitPython sentry-sdk"
   ],
   "metadata": {
    "id": "_iQf5_5HpJvH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8473a5a1-7616-4627-d9a6-793a269b7a6b"
   },
   "id": "_iQf5_5HpJvH",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Skipping pathtools as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping setproctitle as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping docker-pycreds as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e9585",
   "metadata": {
    "id": "c41e9585"
   },
   "source": [
    "## 2) Colab Setup (Install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Data Loading\n"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633aa102",
   "metadata": {
    "id": "633aa102",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "00044861-ce84-42f0-8eb9-4b5b32cd085b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.1/1.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.7/1.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": "# PURPOSE: Load datasets and align schemas\n# - Read ACSIncome, ACSEmployment (Folktables), COMPAS, CivilComments\n# - Select features/target; cast types; attach sensitive attributes for EVAL ONLY\n\n\n#@title Install Runtime Dependencies (CUDA 12.1 stack)\n!pip -q install --upgrade pip\n!pip -q install \"pandas==2.2.2\" \"scikit-learn>=1.4,<1.6\" \"matplotlib>=3.8,<3.10\" \"shap>=0.45,<0.47\"\n!pip -q install \"fairlearn>=0.12,<0.14\" \"folktables>=0.0.12\" \"datasets>=2.20,<2.22\" \"evaluate>=0.4,<0.5\" \"transformers>=4.44,<4.47\"\n!pip -q install --index-url https://download.pytorch.org/whl/cu121 \"torch==2.4.0\" \"torchaudio==2.4.0\" \"torchvision==0.19.0\"\n!pip -q install \"nbformat>=5.10,<6\" \"python-pptx>=0.6.21,<0.7\" \"tabulate>=0.9,<0.10\""
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8c3de0",
   "metadata": {
    "id": "af8c3de0"
   },
   "outputs": [],
   "source": [
    "#@title Conflict Clean-up (safe to run once after installing dependencies)\n",
    "# Removes unused packages that force incompatible pins in Colab.\n",
    "# Run this *after* the Install cell finishes, then go to: Runtime â†’ Restart runtime.\n",
    "!pip -q uninstall -y umap-learn gcsfs\n",
    "!pip -q install \"scikit-learn==1.5.2\" \"fsspec==2024.6.1\" --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13917dc6",
   "metadata": {
    "id": "13917dc6"
   },
   "source": [
    "After installs: **Runtime â†’ Restart runtime**, then run **Environment Check**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7403d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed7403d7",
    "outputId": "d937001b-3368-4d04-e73c-2367eac9bdee"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… torch==2.4.0+cu121\n",
      "âœ… CUDA available: True\n",
      "âœ… Device: Tesla T4\n",
      "âœ… CUDA: 12.1\n",
      "âœ… sklearn==1.5.2\n",
      "âœ… pandas==2.2.2\n",
      "âœ… matplotlib==3.9.4\n",
      "âœ… fairlearn==0.13.0\n",
      "âœ… folktables==0.0.12\n",
      "âœ… datasets==2.21.0\n",
      "âœ… evaluate==0.4.6\n",
      "âœ… transformers==4.46.3\n",
      "âœ… shap==0.46.0\n"
     ]
    }
   ],
   "source": "# PURPOSE: Load datasets and align schemas\n# - Read ACSIncome, ACSEmployment (Folktables), COMPAS, CivilComments\n# - Select features/target; cast types; attach sensitive attributes for EVAL ONLY\n\n\n#@title Environment Check\nimport importlib, torch\n\ndef ok(v): return f\"âœ… {v}\"\ndef warn(v): return f\"âš ï¸ {v}\"\n\ndef check_version(pkg, pref=None):\n    try:\n        m = importlib.import_module(pkg); v = getattr(m, \"__version__\", \"unknown\")\n        return ok(f\"{pkg}=={v}\") if (pref is None or v.startswith(pref)) else warn(f\"{pkg}=={v} (expect {pref})\")\n    except Exception as e:\n        return warn(f\"{pkg} import failed: {e}\")\n\nprint(check_version(\"torch\",\"2.4\"))\nprint(ok(f\"CUDA available: {torch.cuda.is_available()}\"))\nif torch.cuda.is_available():\n    print(ok(f\"Device: {torch.cuda.get_device_name(0)}\")); print(ok(f\"CUDA: {torch.version.cuda}\"))\nfor p, pref in [(\"sklearn\",None),(\"pandas\",\"2.2\"),(\"matplotlib\",None),(\"fairlearn\",None),(\"folktables\",None),(\"datasets\",None),(\"evaluate\",None),(\"transformers\",None),(\"shap\",None)]:\n    print(check_version(p,pref))"
  },
  {
   "cell_type": "markdown",
   "id": "c89dbd9f",
   "metadata": {
    "id": "c89dbd9f"
   },
   "source": [
    "### âœ… What to run next (after Environment Check)\n",
    "1. Run **Utility Functions (metrics, plotting, exports)**  \n",
    "2. **Folktables** â†’ run **ACSIncome** then **ACSEmployment** (baseline â†’ DSAP â†’ Eq-Odds â†’ Threshold Optimizer â†’ plot)  \n",
    "3. **COMPAS** â†’ download/preprocess â†’ methods â†’ plot  \n",
    "4. **CivilComments (Text)** â†’ upload `train.csv` â†’ move & verify â†’ run baseline + plot  \n",
    "5. **FairFace (Optional)** â†’ run toy DeAR demo + plot  \n",
    "6. **Aggregated Results** â†’ write/preview `results_summary_all.csv`  \n",
    "7. **Sanitize & Export** â†’ creates cleaned notebook, release ZIP, and Overleaf pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef483c34",
   "metadata": {
    "id": "ef483c34"
   },
   "source": [
    "## 3) Utility Functions (metrics, plotting, exports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Environment & Imports\n"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa75e1e6",
   "metadata": {
    "id": "aa75e1e6"
   },
   "outputs": [],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Helper: Exponentiated Gradient Probability Extraction\nimport numpy as np\n\ndef eg_probabilities(eg, X):\n    \"\"\"Return class-1 probabilities for ExponentiatedGradient across fairlearn versions.\"\"\"\n    if hasattr(eg, \"_pmf_predict\"):\n        try:\n            pmf = eg._pmf_predict(X)\n            return pmf[:, 1]\n        except Exception:\n            pass\n    if hasattr(eg, \"predictors_\"):\n        probs = []\n        for est in eg.predictors_:\n            if hasattr(est, \"predict_proba\"):\n                probs.append(est.predict_proba(X)[:, 1])\n            elif hasattr(est, \"decision_function\"):\n                s = est.decision_function(X)\n                probs.append(1 / (1 + np.exp(-s)))\n            else:\n                probs.append(est.predict(X).astype(float))\n        P = np.vstack(probs)\n        if hasattr(eg, \"weights_\") and eg.weights_ is not None:\n            w = np.asarray(eg.weights_, dtype=float)\n            w = w / w.sum()\n            return (w @ P).ravel()\n        else:\n            return P.mean(axis=0)\n    return eg.predict(X).astype(float)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb60699",
   "metadata": {
    "id": "beb60699"
   },
   "outputs": [],
   "source": [
    "#@title Helper: Threshold Optimizer Evaluation\n",
    "def evaluate_threshold_optimizer(cal, to, X_test, y_test, g_test, dataset_name):\n",
    "    \"\"\"Evaluate calibrated probabilities + post-processed labels, then log summary.\"\"\"\n",
    "    y_score = cal.predict_proba(X_test)[:, 1]\n",
    "    y_pred  = to.predict(X_test, sensitive_features=g_test)\n",
    "    m_to = evaluate_binary(y_test, y_score, y_pred, g_test)\n",
    "    add_summary_row(dataset_name, \"threshold_optimizer\", m_to)\n",
    "    return m_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3755e612",
   "metadata": {
    "id": "3755e612"
   },
   "outputs": [],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Helper: Trade-off Plot + CSV Export (utility â†‘ vs EO gap â†“)\nimport pandas as pd\ndef export_tradeoff_points(points, dataset_key, title):\n    df = pd.DataFrame(points, columns=[\"label\",\"accuracy\",\"eo_tpr_gap\"]).dropna()\n    csv_path = f\"/content/exports/{dataset_key}_points.csv\"\n    png_path = f\"/content/exports/{dataset_key}_tradeoff.png\"\n    save_results_csv(df, csv_path)\n    tradeoff_plot(df, \"accuracy\", \"eo_tpr_gap\", title, png_path)\n    flush_summary_csv(\"/content/exports/results_summary_all.csv\")\n    return csv_path, png_path"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154146ea",
   "metadata": {
    "id": "154146ea"
   },
   "outputs": [],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title Utilities\nimport os, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nEXPORT_DIR=\"/content/exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n\ndef tpr_fpr(y_true, y_pred, pos=1):\n    y_true=np.asarray(y_true); y_pred=np.asarray(y_pred)\n    tp=np.sum((y_true==pos)&(y_pred==pos)); fn=np.sum((y_true==pos)&(y_pred!=pos))\n    tn=np.sum((y_true!=pos)&(y_pred!=pos)); fp=np.sum((y_true!=pos)&(y_pred==pos))\n    tpr=tp/(tp+fn+1e-12); fpr=fp/(fp+tn+1e-12); tnr=1-fpr; return tpr,fpr,tnr\n\ndef group_gaps(y_true, y_pred, g):\n    import pandas as pd, numpy as np\n    df=pd.DataFrame({\"y\":y_true,\"yp\":y_pred,\"g\":g})\n    tprs, tnrs = [], []\n    for _,sub in df.groupby(\"g\"):\n        tpr, fpr, tnr = tpr_fpr(sub.y, sub.yp); tprs.append(tpr); tnrs.append(tnr)\n    eo = float(np.max(tprs)-np.min(tprs)) if tprs else np.nan\n    tnr_gap = float(np.max(tnrs)-np.min(tnrs)) if tnrs else np.nan\n    return {\"eo_tpr_gap\":eo, \"eqodds_tpr_gap\":eo, \"eqodds_tnr_gap\":tnr_gap}\n\ndef evaluate_binary(y_true, y_score, y_pred, g):\n    out={\"accuracy\": float(accuracy_score(y_true, y_pred))}\n    try: out[\"auc\"]=float(roc_auc_score(y_true, y_score))\n    except: out[\"auc\"]=float(\"nan\")\n    out.update(group_gaps(y_true, y_pred, g)); return out\n\ndef tradeoff_plot(df, x, y, title, out_png):\n    plt.figure(); plt.scatter(df[x], df[y])\n    for _,r in df.iterrows():\n        if isinstance(r.get(\"label\"), str): plt.annotate(r[\"label\"], (r[x], r[y]))\n    plt.xlabel(f\"{x} (â†‘)\"); plt.ylabel(f\"{y} (â†“)\"); plt.title(title); plt.grid(True)\n    plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n\ndef save_results_csv(df, path): df.to_csv(path, index=False)\n\nRESULTS_SUMMARY=[]\ndef add_summary_row(dataset, method, metrics):\n    row={\"dataset\":dataset,\"method\":method}; row.update(metrics); RESULTS_SUMMARY.append(row)\ndef flush_summary_csv(path=\"/content/exports/results_summary_all.csv\"):\n    pd.DataFrame(RESULTS_SUMMARY).to_csv(path, index=False) if RESULTS_SUMMARY else pd.DataFrame([],columns=[\"dataset\",\"method\",\"accuracy\",\"auc\",\"eo_tpr_gap\",\"eqodds_tpr_gap\",\"eqodds_tnr_gap\"]).to_csv(path,index=False)"
  },
  {
   "cell_type": "markdown",
   "id": "526de73a",
   "metadata": {
    "id": "526de73a"
   },
   "source": [
    "## 4) Folktables â€” ACSIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf2f69b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaf2f69b",
    "outputId": "f61a06fc-2b6c-4f4d-9671-c5bfcd04e37a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data for 2018 1-Year person survey for CA...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7845738700247358,\n",
       " 'auc': 0.859781744336534,\n",
       " 'eo_tpr_gap': 0.09241830535443785,\n",
       " 'eqodds_tpr_gap': 0.09241830535443785,\n",
       " 'eqodds_tnr_gap': 0.041503931805421335}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Aligned loaders for Folktables (use task-returned group)\nfrom folktables import ACSDataSource, ACSIncome, ACSEmployment\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\ndef _binarize_group(g_task: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert Folktables' returned group vector to {0,1}.\n    Commonly '1' corresponds to White in RAC1P; we map (==1) -> 1 else 0.\n    If it's already 0/1, we return as-is.\n    \"\"\"\n    g = np.asarray(g_task)\n    if set(np.unique(g)).issubset({0, 1}):\n        return g.astype(int)\n    try:\n        return (g.astype(int) == 1).astype(int)\n    except Exception:\n        return (g > np.median(g)).astype(int)\n\ndef load_acs_income(state=\"CA\", year=2018, horizon=\"1-Year\"):\n    ds = ACSDataSource(survey_year=year, horizon=horizon, survey=\"person\")\n    acs = ds.get_data(download=True, states=[state])\n    X, y, g_task = ACSIncome.df_to_numpy(acs)  # aligned group from task\n    g = _binarize_group(g_task)\n    return X, y, g\n\ndef load_acs_employment(state=\"CA\", year=2018, horizon=\"1-Year\"):\n    ds = ACSDataSource(survey_year=year, horizon=horizon, survey=\"person\")\n    acs = ds.get_data(download=True, states=[state])\n    X, y, g_task = ACSEmployment.df_to_numpy(acs)\n    g = _binarize_group(g_task)\n    return X, y, g\n\ndef split_scale(X, y, g, test_size=0.25, seed=42):\n    Xtr, Xte, ytr, yte, gtr, gte = train_test_split(X, y, g, test_size=test_size, random_state=seed, stratify=y)\n    sc = StandardScaler(with_mean=False)\n    Xtr = sc.fit_transform(Xtr)\n    Xte = sc.transform(Xte)\n    return Xtr, Xte, ytr, yte, gtr, gte, sc\n\n# --- Run income loader and baseline ---\nX, y, g = load_acs_income()\nXtr, Xte, ytr, yte, gtr, gte, _ = split_scale(X, y, g)\n\nbase = LogisticRegression(max_iter=200, n_jobs=-1).fit(Xtr, ytr)\nys = base.predict_proba(Xte)[:, 1]\nyp = (ys >= 0.5).astype(int)\n\nm_base = evaluate_binary(yte, ys, yp, gte)\nadd_summary_row(\"acs_income\", \"baseline\", m_base)\nm_base\n"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87504db9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87504db9",
    "outputId": "a2b7c44e-a4fb-48af-e651-7836a5f2a301"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7785841323057424,\n",
       " 'auc': 0.8587561624177105,\n",
       " 'eo_tpr_gap': 0.018239469510906492,\n",
       " 'eqodds_tpr_gap': 0.018239469510906492,\n",
       " 'eqodds_tnr_gap': 0.00031267896991249433}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title DSAP-style reweighting\nfrom collections import Counter, defaultdict\nimport numpy as np\ndef dsap_reweight(y,g):\n    pairs=list(zip(y,g)); c=Counter(pairs)\n    w=np.array([1.0/c[(yy,gg)] for yy,gg in pairs],dtype=float); return w*(len(w)/w.sum())\nw=dsap_reweight(ytr,gtr)\ndsap=LogisticRegression(max_iter=200,n_jobs=-1).fit(Xtr,ytr,sample_weight=w)\nys2=dsap.predict_proba(Xte)[:,1]; yp2=(ys2>=0.5).astype(int)\nm_dsap=evaluate_binary(yte, ys2, yp2, gte); add_summary_row(\"acs_income\",\"dsap_reweight\",m_dsap); m_dsap"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cdef500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cdef500",
    "outputId": "60521b90-6cf9-446a-cc58-73c2691f4774"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7735552057566899,\n",
       " 'auc': 0.7843569985977736,\n",
       " 'eo_tpr_gap': 0.008965899666668498,\n",
       " 'eqodds_tpr_gap': 0.008965899666668498,\n",
       " 'eqodds_tnr_gap': 0.009792358677063184}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title Equalized Odds via ExponentiatedGradient\nfrom fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\neg=ExponentiatedGradient(DecisionTreeClassifier(max_depth=4,random_state=0), EqualizedOdds(), eps=0.02, max_iter=50)\neg.fit(Xtr,ytr,sensitive_features=gtr)\nys3 = eg_probabilities(eg, Xte)\nyp3=(ys3>=0.5).astype(int)\nm_eg=evaluate_binary(yte, ys3, yp3, gte); add_summary_row(\"acs_income\",\"eq_odds_eg\",m_eg); m_eg"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Baseline Models\n"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56682d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a56682d7",
    "outputId": "9f465c5f-c11c-496c-ca7f-18c4a4ad045b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7793814011488849,\n",
       " 'auc': 0.8598443934980591,\n",
       " 'eo_tpr_gap': 0.013762846228842718,\n",
       " 'eqodds_tpr_gap': 0.013762846228842718,\n",
       " 'eqodds_tnr_gap': 0.00015310303195670283}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": "# PURPOSE: Train baseline models\n# - Tabular: LogisticRegression / tree baseline\n# - Text: DistilBERT fine-tuning for CivilComments\n# - Save validation scores for post-processing\n\n\n#@title Threshold Optimizer (Post-Processing on Calibrated Scores)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom fairlearn.postprocessing import ThresholdOptimizer\nfrom sklearn.model_selection import train_test_split\n\n# Split for calibration and validation\nXc_tr, Xc_val, yc_tr, yc_val, gc_tr, gc_val = train_test_split(\n    Xtr, ytr, gtr, test_size=0.2, random_state=3, stratify=ytr\n)\n\n# 1ï¸âƒ£ Fit the base logistic regression first\nbase_lr = LogisticRegression(max_iter=200, n_jobs=-1)\nbase_lr.fit(Xc_tr, yc_tr)\n\n# 2ï¸âƒ£ Calibrate probabilities (no prefit flag â†’ CalibratedClassifierCV will refit internally)\ncal = CalibratedClassifierCV(base_lr, cv=3, method=\"isotonic\")\ncal.fit(Xc_tr, yc_tr)\n\n# 3ï¸âƒ£ Fit Fairlearn ThresholdOptimizer for Equalized Odds\nto = ThresholdOptimizer(\n    estimator=cal,\n    constraints=\"equalized_odds\",\n    predict_method=\"predict_proba\"\n)\nto.fit(Xc_val, yc_val, sensitive_features=gc_val)\n\n# 4ï¸âƒ£ Evaluate calibrated + post-processed results\nm_to = evaluate_threshold_optimizer(cal, to, Xte, yte, gte, \"acs_income\")\nm_to"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae58cdce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ae58cdce",
    "outputId": "7060c71d-f094-43ae-f6ff-4e9903871562"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/exports/income_tradeoff.png'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title Plot & export (income)\nimport pandas as pd\ndf=pd.DataFrame([\n    {\"label\":\"baseline\",\"accuracy\":m_base[\"accuracy\"],\"eo_tpr_gap\":m_base[\"eo_tpr_gap\"]},\n    {\"label\":\"dsap\",\"accuracy\":m_dsap[\"accuracy\"],\"eo_tpr_gap\":m_dsap[\"eo_tpr_gap\"]},\n    {\"label\":\"eq_odds\",\"accuracy\":m_eg[\"accuracy\"],\"eo_tpr_gap\":m_eg[\"eo_tpr_gap\"]},\n    {\"label\":\"thresh_opt\",\"accuracy\":m_to[\"accuracy\"],\"eo_tpr_gap\":m_to[\"eo_tpr_gap\"]},\n])\npng=\"/content/exports/income_tradeoff.png\"\ntradeoff_plot(df,\"accuracy\",\"eo_tpr_gap\",\"ACSIncome: Utility vs Fairness (EO gap)\",png)\nsave_results_csv(df,\"/content/exports/acs_income_points.csv\"); flush_summary_csv(); png"
  },
  {
   "cell_type": "markdown",
   "id": "88830714",
   "metadata": {
    "id": "88830714"
   },
   "source": [
    "## 5) Folktables â€” ACSEmployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b3ab70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "f0b3ab70",
    "outputId": "af65f1e8-8c15-41dd-b756-e87321147eaf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Baseline metrics:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7716065677630537,\n",
       " 'auc': 0.8452249818660857,\n",
       " 'eo_tpr_gap': 0.017761207445413985,\n",
       " 'eqodds_tpr_gap': 0.017761207445413985,\n",
       " 'eqodds_tnr_gap': 0.02677564559123713}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… DSAP metrics:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7679953539939813,\n",
       " 'auc': 0.8449961915562069,\n",
       " 'eo_tpr_gap': 0.013635294558256295,\n",
       " 'eqodds_tpr_gap': 0.013635294558256295,\n",
       " 'eqodds_tnr_gap': 0.021149626801078902}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Equalized Odds (EG) metrics:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.8087957341217464,\n",
       " 'auc': 0.8359837726968038,\n",
       " 'eo_tpr_gap': 0.043088198943859846,\n",
       " 'eqodds_tpr_gap': 0.043088198943859846,\n",
       " 'eqodds_tnr_gap': 0.030995702234827727}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Threshold Optimizer metrics:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7712369991024761,\n",
       " 'auc': 0.8452943015583347,\n",
       " 'eo_tpr_gap': 0.004291808387610119,\n",
       " 'eqodds_tpr_gap': 0.004291808387610119,\n",
       " 'eqodds_tnr_gap': 0.009960499750997664}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Exported trade-off plot and CSV for ACSEmployment\n"
     ]
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Folktables: ACSEmployment (Tabular Fairness Benchmark)\nfrom folktables import ACSEmployment\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 1ï¸âƒ£ Load and split\nXe, ye, ge = load_acs_employment()\nXtrE, XteE, ytrE, yteE, gtrE, gteE, scE = split_scale(Xe, ye, ge)\n\n# 2ï¸âƒ£ Baseline Logistic Regression\nbaseE = LogisticRegression(max_iter=200, n_jobs=-1).fit(XtrE, ytrE)\nysEb = baseE.predict_proba(XteE)[:, 1]\nypEb = (ysEb >= 0.5).astype(int)\nmEb = evaluate_binary(yteE, ysEb, ypEb, gteE)\nadd_summary_row(\"acs_employment\", \"baseline\", mEb)\nprint(\"âœ… Baseline metrics:\")\ndisplay(mEb)\n\n# 3ï¸âƒ£ DSAP-style reweighting (pre-processing fairness)\nfrom sklearn.utils.class_weight import compute_sample_weight\nwE = compute_sample_weight(\"balanced\", ytrE)\nlr_dsapE = LogisticRegression(max_iter=200, n_jobs=-1)\nlr_dsapE.fit(XtrE, ytrE, sample_weight=wE)\nysEd = lr_dsapE.predict_proba(XteE)[:, 1]\nypEd = (ysEd >= 0.5).astype(int)\nmEd = evaluate_binary(yteE, ysEd, ypEd, gteE)\nadd_summary_row(\"acs_employment\", \"dsap\", mEd)\nprint(\"âœ… DSAP metrics:\")\ndisplay(mEd)\n\n# 4ï¸âƒ£ Equalized Odds (ExponentiatedGradient)\nfrom fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\nfrom sklearn.tree import DecisionTreeClassifier\n\negE = ExponentiatedGradient(\n    DecisionTreeClassifier(max_depth=4, random_state=0),\n    EqualizedOdds(),\n    eps=0.02\n)\negE.fit(XtrE, ytrE, sensitive_features=gtrE)\n\nysEe = eg_probabilities(egE, XteE)\nypEe = (ysEe >= 0.5).astype(int)\nmEe = evaluate_binary(yteE, ysEe, ypEe, gteE)\nadd_summary_row(\"acs_employment\", \"eq_odds_eg\", mEe)\nprint(\"âœ… Equalized Odds (EG) metrics:\")\ndisplay(mEe)\n\n# 5ï¸âƒ£ Threshold Optimizer (Post-Processing on Calibrated Scores)\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom fairlearn.postprocessing import ThresholdOptimizer\n\n# Split training data into calibration (train) and validation subsets\nXc_trE, Xc_valE, yc_trE, yc_valE, gc_trE, gc_valE = train_test_split(\n    XtrE, ytrE, gtrE, test_size=0.2, random_state=3, stratify=ytrE\n)\n\n# Base classifier\nbase_lrE = LogisticRegression(max_iter=200, n_jobs=-1)\nbase_lrE.fit(Xc_trE, yc_trE)\n\n# Calibrate probabilities (cv=3 avoids prefit issues)\ncalE = CalibratedClassifierCV(base_lrE, cv=3, method=\"isotonic\")\ncalE.fit(Xc_trE, yc_trE)\n\n# Threshold Optimizer for Equalized Odds\ntoE = ThresholdOptimizer(\n    estimator=calE,\n    constraints=\"equalized_odds\",\n    predict_method=\"predict_proba\"\n)\ntoE.fit(Xc_valE, yc_valE, sensitive_features=gc_valE)\n\n# Evaluate calibrated + post-processed results\nmE_to = evaluate_threshold_optimizer(calE, toE, XteE, yteE, gteE, \"acs_employment\")\nprint(\"âœ… Threshold Optimizer metrics:\")\ndisplay(mE_to)\n\n# 6ï¸âƒ£ Trade-off plot + CSV export\n_ = export_tradeoff_points(\n    [\n        {\"label\":\"baseline\",   \"accuracy\": mEb[\"accuracy\"], \"eo_tpr_gap\": mEb[\"eo_tpr_gap\"]},\n        {\"label\":\"dsap\",       \"accuracy\": mEd[\"accuracy\"], \"eo_tpr_gap\": mEd[\"eo_tpr_gap\"]},\n        {\"label\":\"eq_odds\",    \"accuracy\": mEe[\"accuracy\"], \"eo_tpr_gap\": mEe[\"eo_tpr_gap\"]},\n        {\"label\":\"thresh_opt\", \"accuracy\": mE_to[\"accuracy\"], \"eo_tpr_gap\": mE_to[\"eo_tpr_gap\"]},\n    ],\n    dataset_key=\"employment\",\n    title=\"ACSEmployment: Utility vs Fairness (EO gap)\"\n)\n\nprint(\"âœ… Exported trade-off plot and CSV for ACSEmployment\")"
  },
  {
   "cell_type": "markdown",
   "id": "5d509c73",
   "metadata": {
    "id": "5d509c73"
   },
   "source": [
    "## 6) COMPAS"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "pT9YQ1kxKAAF"
   },
   "id": "pT9YQ1kxKAAF"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04c3948",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "a04c3948",
    "outputId": "4ebe9c0d-232d-448e-bbee-5542f34c5534"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading COMPAS from ProPublica GitHub...\n",
      "âœ… COMPAS Baseline\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.6757206208425721,\n",
       " 'auc': 0.7235370486903658,\n",
       " 'eo_tpr_gap': 0.27328938056393615,\n",
       " 'eqodds_tpr_gap': 0.27328938056393615,\n",
       " 'eqodds_tnr_gap': 0.15988265043085426}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… COMPAS DSAP\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.6729490022172949,\n",
       " 'auc': 0.7236214491307376,\n",
       " 'eo_tpr_gap': 0.29088911237754655,\n",
       " 'eqodds_tpr_gap': 0.29088911237754655,\n",
       " 'eqodds_tnr_gap': 0.2068511198945976}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… COMPAS Equalized Odds (EG)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.5842572062084257,\n",
       " 'auc': 0.6362849904987445,\n",
       " 'eo_tpr_gap': 0.08850150854844106,\n",
       " 'eqodds_tpr_gap': 0.08850150854844106,\n",
       " 'eqodds_tnr_gap': 0.002798713573760825}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… COMPAS Threshold Optimizer\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'accuracy': 0.656319290465632,\n",
       " 'auc': 0.7227681358549206,\n",
       " 'eo_tpr_gap': 0.03246172756732624,\n",
       " 'eqodds_tpr_gap': 0.03246172756732624,\n",
       " 'eqodds_tnr_gap': 0.01738557598671009}"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Exported: /content/exports/compas_points.csv and /content/exports/compas_tradeoff.png\n"
     ]
    }
   ],
   "source": "# PURPOSE: Load datasets and align schemas\n# - Read ACSIncome, ACSEmployment (Folktables), COMPAS, CivilComments\n# - Select features/target; cast types; attach sensitive attributes for EVAL ONLY\n\n\n#@title COMPAS: Auto-download, preprocess, fairness experiments, plot + export\nimport os, pandas as pd, numpy as np\nfrom urllib.request import urlretrieve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.model_selection import train_test_split\nfrom fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\nfrom fairlearn.postprocessing import ThresholdOptimizer\nfrom sklearn.tree import DecisionTreeClassifier\n\n# ---------- 1) Download & Load ----------\ncompas_dir = \"/content/data/compas\"\nos.makedirs(compas_dir, exist_ok=True)\ncompas_csv = os.path.join(compas_dir, \"compas-scores-two-years.csv\")\n\nif not os.path.exists(compas_csv):\n    print(\"Downloading COMPAS from ProPublica GitHub...\")\n    url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n    urlretrieve(url, compas_csv)\n\ndf = pd.read_csv(compas_csv)\n\n# ---------- 2) Minimal, reproducible preprocessing ----------\n# Keep common fields used in many public baselines\ncols_keep = [\n    \"age\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\",\n    \"priors_count\", \"c_charge_degree\", \"race\", \"sex\", \"two_year_recid\"\n]\ndf = df[cols_keep].dropna()\n\n# Label: two_year_recid (1=reoffended within two years)\nyC = df[\"two_year_recid\"].astype(int).values\n\n# Sensitive attribute: race (African-American=1, others=0)\ngC = (df[\"race\"].astype(str).str.strip() == \"African-American\").astype(int).values\n\n# Features: numeric + one-hot for c_charge_degree and sex\nX_num = df[[\"age\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"priors_count\"]].astype(float)\nX_cat = pd.get_dummies(df[[\"c_charge_degree\", \"sex\"]].astype(str), drop_first=True)\nXC = pd.concat([X_num, X_cat], axis=1).values\n\n# ---------- 3) Train/test split + scaling (uses your split_scale helper) ----------\nXtrC, XteC, ytrC, yteC, gtrC, gteC, scC = split_scale(XC, yC, gC)\n\n# ---------- 4) Baseline ----------\nbaseC = LogisticRegression(max_iter=200, n_jobs=-1)\nbaseC.fit(XtrC, ytrC)\nysCb = baseC.predict_proba(XteC)[:, 1]\nypCb = (ysCb >= 0.5).astype(int)\nmCb = evaluate_binary(yteC, ysCb, ypCb, gteC)\nadd_summary_row(\"compas\", \"baseline\", mCb)\nprint(\"âœ… COMPAS Baseline\"); display(mCb)\n\n# ---------- 5) DSAP-style reweighting (pre) ----------\nwC = compute_sample_weight(\"balanced\", ytrC)\nlr_dsapC = LogisticRegression(max_iter=200, n_jobs=-1)\nlr_dsapC.fit(XtrC, ytrC, sample_weight=wC)\nysCd = lr_dsapC.predict_proba(XteC)[:, 1]\nypCd = (ysCd >= 0.5).astype(int)\nmCd = evaluate_binary(yteC, ysCd, ypCd, gteC)\nadd_summary_row(\"compas\", \"dsap\", mCd)\nprint(\"âœ… COMPAS DSAP\"); display(mCd)\n\n# ---------- 6) Equalized Odds via ExponentiatedGradient (in) ----------\negC = ExponentiatedGradient(\n    DecisionTreeClassifier(max_depth=4, random_state=0),\n    EqualizedOdds(),\n    eps=0.02\n)\negC.fit(XtrC, ytrC, sensitive_features=gtrC)\nysCe = eg_probabilities(egC, XteC)          # uses your helper\nypCe = (ysCe >= 0.5).astype(int)\nmCe = evaluate_binary(yteC, ysCe, ypCe, gteC)\nadd_summary_row(\"compas\", \"eq_odds_eg\", mCe)\nprint(\"âœ… COMPAS Equalized Odds (EG)\"); display(mCe)\n\n# ---------- 7) Threshold Optimizer on Calibrated Scores (post) ----------\n# Split a small calibration/validation fold from the training split\nXc_trC, Xc_valC, yc_trC, yc_valC, gc_trC, gc_valC = train_test_split(\n    XtrC, ytrC, gtrC, test_size=0.2, random_state=3, stratify=ytrC\n)\n\n# Base classifier for calibration\nbase_lrC = LogisticRegression(max_iter=200, n_jobs=-1)\nbase_lrC.fit(Xc_trC, yc_trC)\n\n# Calibrator (cv=3 -> no prefit headaches)\nfrom sklearn.calibration import CalibratedClassifierCV\ncalC = CalibratedClassifierCV(base_lrC, cv=3, method=\"isotonic\")\ncalC.fit(Xc_trC, yc_trC)\n\n# Threshold Optimizer for Equalized Odds\ntoC = ThresholdOptimizer(\n    estimator=calC,\n    constraints=\"equalized_odds\",\n    predict_method=\"predict_proba\"\n)\ntoC.fit(Xc_valC, yc_valC, sensitive_features=gc_valC)\n\n# Evaluate using your helper (calibrated probs + post-processed labels)\nmC_to = evaluate_threshold_optimizer(calC, toC, XteC, yteC, gteC, \"compas\")\nprint(\"âœ… COMPAS Threshold Optimizer\"); display(mC_to)\n\n# ---------- 8) Trade-off plot + CSV export ----------\n_ = export_tradeoff_points(\n    [\n        {\"label\":\"baseline\",   \"accuracy\": mCb[\"accuracy\"],  \"eo_tpr_gap\": mCb[\"eo_tpr_gap\"]},\n        {\"label\":\"dsap\",       \"accuracy\": mCd[\"accuracy\"],  \"eo_tpr_gap\": mCd[\"eo_tpr_gap\"]},\n        {\"label\":\"eq_odds\",    \"accuracy\": mCe[\"accuracy\"],  \"eo_tpr_gap\": mCe[\"eo_tpr_gap\"]},\n        {\"label\":\"thresh_opt\", \"accuracy\": mC_to[\"accuracy\"],\"eo_tpr_gap\": mC_to[\"eo_tpr_gap\"]},\n    ],\n    dataset_key=\"compas\",\n    title=\"COMPAS: Utility vs Fairness (EO gap)\"\n)\n\nprint(\"âœ… Exported: /content/exports/compas_points.csv and /content/exports/compas_tradeoff.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd70871",
   "metadata": {
    "id": "5bd70871"
   },
   "outputs": [],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Methods + plot (COMPAS) â€” fixed & consistent\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.model_selection import train_test_split\nfrom fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\nfrom fairlearn.postprocessing import ThresholdOptimizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport numpy as np\nimport pandas as pd\n\n# ---------- Baseline ----------\nbC = LogisticRegression(max_iter=400, n_jobs=-1).fit(XtrC, ytrC)\nysC = bC.predict_proba(XteC)[:, 1]\nypC = (ysC >= 0.5).astype(int)\nmCb = evaluate_binary(yteC, ysC, ypC, gteC)\nadd_summary_row(\"compas\", \"baseline\", mCb)\n\n# ---------- DSAP-style reweighting (pre) ----------\nwC = compute_sample_weight(\"balanced\", ytrC)  # simple DSAP-style weight\ndC = LogisticRegression(max_iter=400, n_jobs=-1).fit(XtrC, ytrC, sample_weight=wC)\nysCd = dC.predict_proba(XteC)[:, 1]\nypCd = (ysCd >= 0.5).astype(int)\nmCd = evaluate_binary(yteC, ysCd, ypCd, gteC)\nadd_summary_row(\"compas\", \"dsap\", mCd)\n\n# ---------- Equalized Odds (ExponentiatedGradient) ----------\negC = ExponentiatedGradient(\n    DecisionTreeClassifier(max_depth=4, random_state=0),\n    EqualizedOdds(),\n    eps=0.02\n)\negC.fit(XtrC, ytrC, sensitive_features=gtrC)\nysCe = eg_probabilities(egC, XteC)      # helper you added\nypCe = (ysCe >= 0.5).astype(int)\nmCe = evaluate_binary(yteC, ysCe, ypCe, gteC)\nadd_summary_row(\"compas\", \"eq_odds_eg\", mCe)\n\n# ---------- Threshold Optimizer on calibrated scores (post) ----------\n# Split off validation from train for threshold optimization\nXc_trC, Xc_valC, yc_trC, yc_valC, gc_trC, gc_valC = train_test_split(\n    XtrC, ytrC, gtrC, test_size=0.2, random_state=3, stratify=ytrC\n)\n\n# Fit base model for calibration\nbase_lrC = LogisticRegression(max_iter=400, n_jobs=-1)\nbase_lrC.fit(Xc_trC, yc_trC)\n\n# Calibrate probabilities (cv=3 avoids prefit errors)\ncalC = CalibratedClassifierCV(base_lrC, cv=3, method=\"isotonic\")\ncalC.fit(Xc_trC, yc_trC)\n\n# Fairlearn Threshold Optimizer (Equalized Odds)\ntoC = ThresholdOptimizer(\n    estimator=calC,\n    constraints=\"equalized_odds\",\n    predict_method=\"predict_proba\"\n)\ntoC.fit(Xc_valC, yc_valC, sensitive_features=gc_valC)\n\n# Evaluate via helper: calibrated probs + EO labels\nmCt = evaluate_threshold_optimizer(calC, toC, XteC, yteC, gteC, \"compas\")\n\n# ---------- Trade-off plot + CSV export (standardized filenames) ----------\n_ = export_tradeoff_points(\n    [\n        {\"label\":\"baseline\",   \"accuracy\": mCb[\"accuracy\"], \"eo_tpr_gap\": mCb[\"eo_tpr_gap\"]},\n        {\"label\":\"dsap\",       \"accuracy\": mCd[\"accuracy\"], \"eo_tpr_gap\": mCd[\"eo_tpr_gap\"]},\n        {\"label\":\"eq_odds\",    \"accuracy\": mCe[\"accuracy\"], \"eo_tpr_gap\": mCe[\"eo_tpr_gap\"]},\n        {\"label\":\"thresh_opt\", \"accuracy\": mCt[\"accuracy\"], \"eo_tpr_gap\": mCt[\"eo_tpr_gap\"]},\n    ],\n    dataset_key=\"compas\",\n    title=\"COMPAS: Utility vs Fairness (EO gap)\"\n)"
  },
  {
   "cell_type": "markdown",
   "id": "6a165dfc",
   "metadata": {
    "id": "6a165dfc"
   },
   "source": [
    "## 7) CivilComments â€” manual upload + baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c05248",
   "metadata": {
    "id": "70c05248"
   },
   "source": [
    "Steps: Upload `train.csv` â†’ move to `/content/data/civilcomments/train.csv` â†’ verify â†’ run model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86bf9a10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86bf9a10",
    "outputId": "225dc388-7841-4d4d-a2b9-bf1ab2666768"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exists: True /content/data/civilcomments/train.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Move uploaded train.csv\n",
    "import os, shutil\n",
    "src=\"/content/train.csv\"; dst_dir=\"/content/data/civilcomments\"; dst=f\"{dst_dir}/train.csv\"\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "if os.path.exists(src): shutil.move(src, dst)\n",
    "print(\"Exists:\", os.path.exists(dst), dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea0c854",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ea0c854",
    "outputId": "1f8920d9-0baf-4015-d94b-422e795fabb8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 797088\n",
      "-rw-r--r-- 1 root root 816211476 Nov  4 02:50 train.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Verify\n",
    "!ls -l /content/data/civilcomments || echo \"Missing directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2ceb24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "76f2f858620441df84599353991dfe10",
      "5c2078e85b024c848a34b7f951c5f0a1",
      "952949e2a548412bb7e18e414b2de542",
      "31bf55b848014554a7ae511444269d93",
      "8623c42912524020b504e935bbb7a122",
      "11527b9f748b423fbf8eddf131853d32",
      "023b225d7e0448b0a237efb486ca6f3d",
      "7e18edc7c1b5494f91b5b7a66d4b7e23",
      "6a7841bb2655440b92666f1269d2689b",
      "b055bd24941749d994f023a13d105bed",
      "d9cb31252b9c4d28b2e7a0bc522d4d53",
      "26b96b4e0cf448f5ad40e6158d7b55e8",
      "6f655b8a3dd04d85bb0656e67d6f74b0",
      "4b8aafe43907417598b3ecedd57d7f58",
      "be65ff868dfe4e22baa85343c065f95c",
      "fc1007d82d6a4f37ac72f4e6a8c56b27",
      "db72e5642a934de6b265d3e118fd3700",
      "9e32d2a72adc40a6ab02b744d5d4f523",
      "e7c593ba83be46ed9859e3cde64a36ad",
      "5b5729a23eaa4c2cbe08707acf501ebc",
      "33237f36263646d68900c0b6074c93d9",
      "b54d59dabcbb4c32b57c4a7c6e29fab3"
     ]
    },
    "id": "9f2ceb24",
    "outputId": "38168f7e-3e5c-4a4d-847f-2e1892b89050"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76f2f858620441df84599353991dfe10"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26b96b4e0cf448f5ad40e6158d7b55e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipython-input-800341697.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.175457</td>\n",
       "      <td>0.939167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/exports/civilcomments_tradeoff.png'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Baseline DistilBERT + trade-off plot (W&B disabled, Trainer.predict eval)\n\n# ðŸš« Disable Weights & Biases before importing transformers (belt & suspenders)\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\nos.environ[\"DISABLE_WANDB\"] = \"true\"\n\nimport pandas as pd, numpy as np, torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# --- Load data ---\np = \"/content/data/civilcomments/train.csv\"\nif not os.path.exists(p):\n    raise FileNotFoundError(\"Upload train.csv first (see instructions cell).\")\n\ndf = pd.read_csv(p)\ntext_col = \"comment_text\" if \"comment_text\" in df.columns else \"text\"\nlabel_col = \"target\" if \"target\" in df.columns else (\"toxicity\" if \"toxicity\" in df.columns else None)\nassert text_col in df.columns and label_col in df.columns, \"Need text/comment_text and target/toxicity columns.\"\n\n# Binary label: >= 0.5 -> toxic\ndf = df[[text_col, label_col]].rename(columns={text_col: \"text\", label_col: \"label\"}).dropna()\ndf[\"label\"] = (df[\"label\"] >= 0.5).astype(int)\n\n# Small sample for speed\ndf = df.sample(n=min(12000, len(df)), random_state=42)\n\ntr, te = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n\n# --- Tokenization ---\ntok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndef tokf(batch):\n    return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\nds_tr = Dataset.from_pandas(tr).map(tokf, batched=True).rename_column(\"label\", \"labels\")\nds_te = Dataset.from_pandas(te).map(tokf, batched=True).rename_column(\"label\", \"labels\")\nds_tr.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nds_te.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# --- Model ---\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# Trainer metrics helper (expects numpy arrays)\ndef cm(eval_pred):\n    logits, labels = eval_pred\n    if isinstance(logits, tuple):  # some HF versions\n        logits = logits[0]\n    preds = (logits[:, 1] >= 0.5).astype(int)\n    return {\"accuracy\": accuracy_score(labels, preds)}\n\n# --- TrainingArguments: no reporting/logging that triggers W&B ---\nargs = TrainingArguments(\n    output_dir=\"/content/cc_out\",\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=3e-5,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    logging_strategy=\"no\",\n    report_to=\"none\",      # <- critical: disables wandb/tensorboard/mlflow\n    run_name=None,\n    load_best_model_at_end=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds_tr,\n    eval_dataset=ds_te,\n    tokenizer=tok,\n    compute_metrics=cm,\n)\n\n# --- Train ---\ntrainer.train()\n\n# --- Evaluate using Trainer.predict (handles device automatically) ---\npred = trainer.predict(ds_te)\nlogits = pred.predictions if not isinstance(pred.predictions, tuple) else pred.predictions[0]\nlabels = pred.label_ids\n\nprobs = torch.softmax(torch.from_numpy(logits), dim=-1)[:, 1].numpy()\ny_score = probs\ny_pred  = (y_score >= 0.5).astype(int)\ng_test  = np.zeros(len(y_pred), dtype=int)   # no group labels available for CivilComments baseline\n\n# --- Metrics, export, plot ---\nm_cc = evaluate_binary(labels, y_score, y_pred, g_test)\nadd_summary_row(\"civilcomments\", \"baseline_distilbert\", m_cc)\n\ndfp = pd.DataFrame([{\"label\": \"baseline\", \"accuracy\": m_cc[\"accuracy\"], \"eo_tpr_gap\": m_cc[\"eo_tpr_gap\"]}])\npng = \"/content/exports/civilcomments_tradeoff.png\"\ntradeoff_plot(dfp, \"accuracy\", \"eo_tpr_gap\", \"CivilComments: Utility vs Fairness (EO gap)\", png)\nsave_results_csv(dfp, \"/content/exports/civilcomments_points.csv\")\nflush_summary_csv()\npng\n"
  },
  {
   "cell_type": "markdown",
   "id": "5d1e7f88",
   "metadata": {
    "id": "5d1e7f88"
   },
   "source": [
    "## 8) Vision (optional) â€” DeAR (toy) + FRAME/SLSD stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Exports (Figures + CSV + Overleaf)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156cbb14",
   "metadata": {
    "id": "156cbb14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "94e1b3be-d661-4a9f-bb15-e777c2bd7c95"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset ready: True\n"
     ]
    }
   ],
   "source": "# PURPOSE: Export artifacts for paper and slides\n# - Save tradeoff plots (income/employment/compas/civilcomments)\n# - Save consolidated results_summary_all.csv\n# - (Optional) Package Overleaf zip\n\n\n#@title FairFace-like tiny subset (placeholders)\nimport os, pandas as pd, numpy as np\nfrom PIL import Image\nFAIR_DIR=\"/content/data/fairface\"; os.makedirs(FAIR_DIR, exist_ok=True)\nmeta=f\"{FAIR_DIR}/subset.csv\"\nif not os.path.exists(meta):\n    rows=[]\n    for i in range(60):\n        img=Image.new(\"RGB\",(64,64),(50+3*i%255,100+5*i%255,150+7*i%255))\n        p=f\"{FAIR_DIR}/img_{i}.png\"; img.save(p); rows.append({\"img_path\":p})\n    pd.DataFrame(rows).to_csv(meta, index=False)\nprint(\"Subset ready:\", os.path.exists(meta))"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d40adf4",
   "metadata": {
    "id": "0d40adf4"
   },
   "outputs": [],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title DeAR residualization on embeddings (toy)\nimport numpy as np, pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nmeta=\"/content/data/fairface/subset.csv\"\nif not os.path.exists(meta):\n    print(\"Skipping DeAR demo.\")\nelse:\n    df=pd.read_csv(meta)\n    Z=[]\n    for p in df[\"img_path\"]:\n        arr=np.array(Image.open(p)).astype(np.float32)/255.0\n        Z.append([arr.mean(), arr.std(), arr[...,0].mean(), arr[...,1].mean(), arr[...,2].mean()])\n    Z=np.asarray(Z); y=(Z[:,0]>np.median(Z[:,0])).astype(int); g=(Z[:,3]>np.median(Z[:,3])).astype(int)\n    Xtr,Xte,ytr,yte,gtr,gte=train_test_split(Z,y,g,test_size=0.3,random_state=1,stratify=y)\n    sc=StandardScaler(); Xtr=sc.fit_transform(Xtr); Xte=sc.transform(Xte)\n    base=LogisticRegression().fit(Xtr,ytr); ys=base.predict_proba(Xte)[:,1]; yp=(ys>=0.5).astype(int)\n    m_b=evaluate_binary(yte, ys, yp, gte); add_summary_row(\"fairface\",\"baseline_toy\",m_b)\n    def residualize(Z,g):\n        Zr=Z.copy().astype(float); A=g.reshape(-1,1); ATA=np.linalg.pinv(A.T@A + 1e-6)\n        for j in range(Z.shape[1]):\n            z=Z[:,j:j+1]; beta=ATA @ (A.T@z); Zr[:,j]=(z - A@beta).ravel()\n        return Zr\n    Zr=residualize(Z,g); Xtr2,Xte2,ytr2,yte2,gtr2,gte2=train_test_split(Zr,y,g,test_size=0.3,random_state=1,stratify=y)\n    Xtr2=sc.fit_transform(Xtr2); Xte2=sc.transform(Xte2)\n    dear=LogisticRegression().fit(Xtr2,ytr2); ys2=dear.predict_proba(Xte2)[:,1]; yp2=(ys2>=0.5).astype(int)\n    m_d=evaluate_binary(yte2, ys2, yp2, gte2); add_summary_row(\"fairface\",\"dear_residual\",m_d)\n    import pandas as pd\n    dfv=pd.DataFrame([{\"label\":\"baseline\",\"accuracy\":m_b[\"accuracy\"],\"eo_tpr_gap\":m_b[\"eo_tpr_gap\"]},\n                      {\"label\":\"dear\",\"accuracy\":m_d[\"accuracy\"],\"eo_tpr_gap\":m_d[\"eo_tpr_gap\"]}])\n    png=\"/content/exports/fairface_dear_tradeoff.png\"; tradeoff_plot(dfv,\"accuracy\",\"eo_tpr_gap\",\"FairFace (toy): Utility vs Fairness\",png)\n    save_results_csv(dfv,\"/content/exports/fairface_points.csv\"); flush_summary_csv(); png"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74f62f0e",
   "metadata": {
    "id": "74f62f0e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c746b61e-9a93-4d25-a0ed-7ebb0c420826"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FRAME/SLSD stubs loaded.\n"
     ]
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title FRAME & SLSD stubs\nFRAME_DESCRIPTION=\"FRAME: augmentation/ensemble-aware reweighting placeholder.\"\nSLSD_DESCRIPTION=\"SLSD: latent transform neutralizing single sensitive direction (placeholder).\"\ndef FRAME_stub_sample_weights(X,y,g,policy=\"uniform\"):\n    import numpy as np; return np.ones(len(y),dtype=float)\ndef SLSD_stub_transform(Z,g): return Z\nprint(\"FRAME/SLSD stubs loaded.\")"
  },
  {
   "cell_type": "markdown",
   "id": "787f28bd",
   "metadata": {
    "id": "787f28bd"
   },
   "source": [
    "## 9) Aggregated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aead56e2",
   "metadata": {
    "id": "aead56e2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "outputId": "db3bf956-5530-4ed7-96e1-1480bc92d9b4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          dataset               method  accuracy       auc  eo_tpr_gap  \\\n",
       "0      acs_income             baseline  0.784574  0.859782    0.092418   \n",
       "1      acs_income        dsap_reweight  0.778584  0.858756    0.018239   \n",
       "2      acs_income           eq_odds_eg  0.773555  0.784357    0.008966   \n",
       "3      acs_income  threshold_optimizer  0.779381  0.859844    0.013763   \n",
       "4  acs_employment             baseline  0.771607  0.845225    0.017761   \n",
       "\n",
       "   eqodds_tpr_gap  eqodds_tnr_gap  \n",
       "0        0.092418        0.041504  \n",
       "1        0.018239        0.000313  \n",
       "2        0.008966        0.009792  \n",
       "3        0.013763        0.000153  \n",
       "4        0.017761        0.026776  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-5dada4f4-1de6-4232-9f99-7f24664e4d91\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>eo_tpr_gap</th>\n",
       "      <th>eqodds_tpr_gap</th>\n",
       "      <th>eqodds_tnr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acs_income</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.784574</td>\n",
       "      <td>0.859782</td>\n",
       "      <td>0.092418</td>\n",
       "      <td>0.092418</td>\n",
       "      <td>0.041504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acs_income</td>\n",
       "      <td>dsap_reweight</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>0.858756</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acs_income</td>\n",
       "      <td>eq_odds_eg</td>\n",
       "      <td>0.773555</td>\n",
       "      <td>0.784357</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.009792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acs_income</td>\n",
       "      <td>threshold_optimizer</td>\n",
       "      <td>0.779381</td>\n",
       "      <td>0.859844</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acs_employment</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.771607</td>\n",
       "      <td>0.845225</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>0.026776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dada4f4-1de6-4232-9f99-7f24664e4d91')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5dada4f4-1de6-4232-9f99-7f24664e4d91 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5dada4f4-1de6-4232-9f99-7f24664e4d91');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-465490a5-3b07-4d77-be3d-65cd34f6a07f\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-465490a5-3b07-4d77-be3d-65cd34f6a07f')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-465490a5-3b07-4d77-be3d-65cd34f6a07f button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"import pandas as pd; pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"acs_employment\",\n          \"acs_income\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"dsap_reweight\",\n          \"threshold_optimizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00512446341051362,\n        \"min\": 0.7716065677630537,\n        \"max\": 0.7845738700247358,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7785841323057424,\n          0.7716065677630537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.032587072122335796,\n        \"min\": 0.7843569985977736,\n        \"max\": 0.8598443934980591,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8587561624177105,\n          0.8452249818660857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eo_tpr_gap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03496411748962323,\n        \"min\": 0.0089658996666684,\n        \"max\": 0.0924183053544378,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0182394695109064,\n          0.0177612074454139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eqodds_tpr_gap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03496411748962323,\n        \"min\": 0.0089658996666684,\n        \"max\": 0.0924183053544378,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0182394695109064,\n          0.0177612074454139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eqodds_tnr_gap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018040845442836646,\n        \"min\": 0.0001531030319567,\n        \"max\": 0.0415039318054213,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0003126789699124,\n          0.0267756455912371\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n\n#@title Write CSV & preview\nflush_summary_csv(\"/content/exports/results_summary_all.csv\")\nimport pandas as pd; pd.read_csv(\"/content/exports/results_summary_all.csv\").head()"
  },
  {
   "cell_type": "markdown",
   "id": "9792676c",
   "metadata": {
    "id": "9792676c"
   },
   "source": [
    "## 10) Export & Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ede1d9c",
   "metadata": {
    "id": "3ede1d9c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9b9791df-c9d5-4ce4-c293-7dc7cdcf2b05"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Save notebook first so it appears in CWD.\n",
      "Created: /content/AIM460_release_bundle.zip\n",
      "Created: /content/overleaf_pack.zip\n"
     ]
    }
   ],
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n#@title Sanitize, release bundle, Overleaf pack\nimport os, glob, shutil, zipfile, nbformat, pandas as pd\n\nSANITIZED=\"/content/AIM460_Fairness_Benchmark_Final_clean.ipynb\"\nRELEASE=\"/content/release_bundle\"; OVERLEAF=\"/content/overleaf_pack\"; EXPORTS=\"/content/exports\"\nos.makedirs(RELEASE, exist_ok=True); os.makedirs(OVERLEAF, exist_ok=True); os.makedirs(EXPORTS, exist_ok=True)\n\n# Sanitize: remove widgets metadata from the most recent .ipynb in CWD\ncands=sorted(glob.glob(\"*.ipynb\"), key=os.path.getmtime, reverse=True)\nif cands:\n    nb=nbformat.read(cands[0], as_version=4); nb.metadata.pop(\"widgets\", None)\n    with open(SANITIZED,\"w\") as f: nbformat.write(nb,f); print(\"Sanitized:\", SANITIZED)\nelse:\n    print(\"Save notebook first so it appears in CWD.\")\n\n# Copy exports + sanitized into release\nif os.path.exists(SANITIZED): shutil.copy(SANITIZED, RELEASE)\nfor p in glob.glob(f\"{EXPORTS}/*\"): shutil.copy(p, RELEASE)\n\n# Slides (pptx) placeholders\nfrom pptx import Presentation\nprs=Presentation()\ns=prs.slides.add_slide(prs.slide_layouts[0]); s.shapes.title.text=\"Cross-Domain Algorithmic Fairness Benchmark\"; s.placeholders[1].text=\"AIM460 Capstone\"\ndef slide(title, bullets, notes=None):\n    sl=prs.slides.add_slide(prs.slide_layouts[1]); sl.shapes.title.text=title; tf=sl.placeholders[1].text_frame; tf.clear()\n    for i,b in enumerate(bullets):\n        p=tf.add_paragraph() if i else tf.paragraphs[0]; p.text=b\n    if notes: sl.notes_slide.notes_text_frame.text=notes\nslide(\"Methods\", [\"DSAP (pre)\", \"Eq-Odds (EG)\", \"Threshold Optimizer\", \"FRAME/SLSD stubs\"], \"Axes: Accuracy â†‘; EO gap â†“\")\nslide(\"Datasets\", [\"ACSIncome\", \"ACSEmployment\", \"COMPAS\", \"CivilComments\", \"FairFace (opt)\"])\nslide(\"Tabular Results\", [\"income_tradeoff.png\", \"employment_tradeoff.png\"])\nslide(\"COMPAS\", [\"compas_tradeoff.png\"])\nslide(\"Text & Vision\", [\"civilcomments_tradeoff.png\", \"fairface_dear_tradeoff.png\"])\nslide(\"Key Takeaways\", [\"No single method dominates\", \"Context matters\", \"Calibration helps post-processing\"])\nslides_path=os.path.join(RELEASE,\"AIM460_slides.pptx\"); prs.save(slides_path)\n\n# Overleaf: write files without raw string issues\nmain_lines=[\n    r\"\\documentclass[conference]{IEEEtran}\",\n    r\"\\usepackage{graphicx}\",\n    r\"\\usepackage{booktabs}\",\n    r\"\\usepackage{hyperref}\",\n    r\"\\begin{document}\",\n    r\"\\title{Cross-Domain Algorithmic Fairness Benchmark}\",\n    r\"\\author{AIM460 Capstone}\",\n    r\"\\maketitle\",\n    r\"\\begin{abstract}\",\n    r\"We present a cross-domain benchmark evaluating pre-, in-, and post-processing fairness methods across multiple modalities.\",\n    r\"\\end{abstract}\",\n    r\"\\section{Introduction}\",\n    r\"% \\cite{intro}\",\n    r\"\\section{Related Work}\",\n    r\"% \\cite{fairlearn,frame,dsap,slsd,dear}\",\n    r\"\\section{Methods}\",\n    r\"Pre: DSAP. In: Equalized Odds (Exponentiated Gradient). Post: Threshold Optimizer (calibrated). FRAME/SLSD described and stubbed.\",\n    r\"\\section{Datasets}\",\n    r\"ACSIncome, ACSEmployment, COMPAS, CivilComments, FairFace (toy).\",\n    r\"\\section{Experimental Setup}\",\n    r\"Splits, calibration, sensitive groups.\",\n    r\"\\section{Results}\",\n    r\"See Figs.~\\ref{fig:inc}, \\ref{fig:emp}, \\ref{fig:compas}, \\ref{fig:civil}, \\ref{fig:fairface}.\",\n    r\"\\begin{figure}[t]\\centering\\includegraphics[width=0.9\\linewidth]{income_tradeoff.png}\\caption{ACSIncome: Accuracy (\\uparrow) vs EO gap (\\downarrow).}\\label{fig:inc}\\end{figure}\",\n    r\"\\begin{figure}[t]\\centering\\includegraphics[width=0.9\\linewidth]{employment_tradeoff.png}\\caption{ACSEmployment: Accuracy (\\uparrow) vs EO gap (\\downarrow).}\\label{fig:emp}\\end{figure}\",\n    r\"\\begin{figure}[t]\\centering\\includegraphics[width=0.9\\linewidth]{compas_tradeoff.png}\\caption{COMPAS: Accuracy (\\uparrow) vs EO gap (\\downarrow).}\\label{fig:compas}\\end{figure}\",\n    r\"\\begin{figure}[t]\\centering\\includegraphics[width=0.9\\linewidth]{civilcomments_tradeoff.png}\\caption{CivilComments: Accuracy (\\uparrow) vs EO gap (\\downarrow).}\\label{fig:civil}\\end{figure}\",\n    r\"\\begin{figure}[t]\\centering\\includegraphics[width=0.9\\linewidth]{fairface_dear_tradeoff.png}\\caption{FairFace: Accuracy (\\uparrow) vs EO gap (\\downarrow).}\\label{fig:fairface}\\end{figure}\",\n    r\"\\input{results_summary_all.tex}\",\n    r\"\\section{Discussion}\",\n    r\"Utility/fairness trade-offs across domains.\",\n    r\"\\section{Limitations}\",\n    r\"Proxies for sensitive groups; small subsamples for speed.\",\n    r\"\\section{Conclusion}\",\n    r\"Summary and future work.\",\n    r\"\\bibliographystyle{IEEEtran}\",\n    r\"\\bibliography{references}\",\n    r\"\\end{document}\",\n]\nos.makedirs(OVERLEAF, exist_ok=True)\nwith open(os.path.join(OVERLEAF,\"main.tex\"),\"w\") as f: f.write(\"\\n\".join(main_lines))\n\ncsv=\"/content/exports/results_summary_all.csv\"\nwith open(os.path.join(OVERLEAF,\"results_summary_all.tex\"),\"w\") as f:\n    if os.path.exists(csv):\n        import pandas as pd; df=pd.read_csv(csv); f.write(df.to_latex(index=False, float_format=\"%.3f\"))\n    else:\n        f.write(\"% Generate results CSV first.\\n\")\n\nwith open(os.path.join(OVERLEAF,\"references.bib\"),\"w\") as f: f.write(\"% Add your BibTeX entries here.\\n\")\n\n# Copy exports into Overleaf\nfor p in glob.glob(f\"{EXPORTS}/*\"): shutil.copy(p, OVERLEAF)\n\n# ZIPs\ndef make_zip(root, out):\n    with zipfile.ZipFile(out, \"w\", zipfile.ZIP_DEFLATED) as z:\n        for folder,_,files in os.walk(root):\n            for fn in files:\n                fp=os.path.join(folder, fn); z.write(fp, os.path.relpath(fp, root))\n\nrelease_zip=\"/content/AIM460_release_bundle.zip\"; overleaf_zip=\"/content/overleaf_pack.zip\"\nmake_zip(RELEASE, release_zip); make_zip(OVERLEAF, overleaf_zip)\nprint(\"Created:\", release_zip); print(\"Created:\", overleaf_zip)"
  },
  {
   "cell_type": "code",
   "source": [
    "# === DIAGNOSE + FIX NOTEBOOK PATHS (pick by index, auto-rename to clean paths) ===\n",
    "import os, glob, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Find all ipynb files under /content (depth 3)\n",
    "candidates = []\n",
    "for pattern in [\"/*.ipynb\",\"/*/*.ipynb\",\"/*/*/*.ipynb\"]:\n",
    "    for p in glob.glob(\"/content\" + pattern):\n",
    "        candidates.append(Path(p))\n",
    "candidates = sorted(set(candidates))\n",
    "\n",
    "print(\"Found these .ipynb files under /content:\\n\")\n",
    "for i,p in enumerate(candidates):\n",
    "    print(f\"[{i}] {p}\")\n",
    "\n",
    "if not candidates:\n",
    "    raise SystemExit(\"No .ipynb files found under /content. If your files are in Drive, mount and copy them:\\n\"\n",
    "                     \"from google.colab import drive; drive.mount('/content/drive')\\n\"\n",
    "                     \"!cp '/content/drive/MyDrive/your/folder/*.ipynb' /content/\")\n",
    "\n",
    "# 2) >>> EDIT THESE TWO INDICES to select your NEWER and OLDER notebooks <<<\n",
    "NEW_IDX = 0   # <-- put the index of Maurrasse_AIM460_Fairness_Benchmark_(1).ipynb\n",
    "OLD_IDX = 1   # <-- put the index of aim460_fairness_project_complete_final_aligned (2).ipynb\n",
    "\n",
    "assert 0 <= NEW_IDX < len(candidates), \"NEW_IDX out of range\"\n",
    "assert 0 <= OLD_IDX < len(candidates), \"OLD_IDX out of range\"\n",
    "NEW_SRC = candidates[NEW_IDX]\n",
    "OLD_SRC = candidates[OLD_IDX]\n",
    "\n",
    "# 3) Copy to clean, canonical names (avoid spaces/parentheses)\n",
    "NEW_DST = Path(\"/content/newer.ipynb\")\n",
    "OLD_DST = Path(\"/content/older.ipynb\")\n",
    "if NEW_DST.exists(): NEW_DST.unlink()\n",
    "if OLD_DST.exists(): OLD_DST.unlink()\n",
    "shutil.copy2(NEW_SRC, NEW_DST)\n",
    "shutil.copy2(OLD_SRC, OLD_DST)\n",
    "\n",
    "print(\"\\nâœ… Canonical paths set:\")\n",
    "print(\"NEW_NB =\", NEW_DST)\n",
    "print(\"OLD_NB =\", OLD_DST)\n",
    "\n",
    "# 4) Sanity check that /content/exports exists\n",
    "exp = Path(\"/content/exports\")\n",
    "print(\"\\nExports folder:\", exp, \"exists?\" , exp.exists())\n",
    "if not exp.exists():\n",
    "    print(\"âš ï¸ /content/exports is missing. Re-run your newer notebook to generate it.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpQiCwdmAyGI",
    "outputId": "b0b9d918-e18c-45e4-8556-751df91e1aba"
   },
   "id": "vpQiCwdmAyGI",
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found these .ipynb files under /content:\n",
      "\n",
      "[0] /content/Maurrasse_AIM460_Fairness_Benchmark (1).ipynb\n",
      "[1] /content/aim460_fairness_project_complete_final_aligned (2).ipynb\n",
      "\n",
      "âœ… Canonical paths set:\n",
      "NEW_NB = /content/newer.ipynb\n",
      "OLD_NB = /content/older.ipynb\n",
      "\n",
      "Exports folder: /content/exports exists? True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n# === ONE-CELL FINALIZER ===\n# Assumes /content/exports/ already exists with PNGs + results_summary_all.csv\n\n# ---- EDIT THESE 2 PATHS IF NEEDED ----\nNEW_NB = \"/content/newer.ipynb\"  # newer, correct\nOLD_NB = \"/content/older.ipynb\"  # older, better documentation\n\n\n# --------------------------------------\n\nimport os, json, re, shutil, subprocess, sys\nfrom pathlib import Path\nimport pandas as pd, numpy as np\n\nroot = Path(\"/content\")\nexports = root / \"exports\"\nassert Path(NEW_NB).exists(), f\"Missing newer notebook at {NEW_NB}\"\nassert Path(OLD_NB).exists(), f\"Missing older notebook at {OLD_NB} (upload it to /content)\"\nassert exports.exists(), \"Missing /content/exports (expected plots + CSVs)\"\n\n# ---------- 1) Release bundle ----------\nbundle_dir = root / \"AIM460_release_bundle\"\nbundle_zip = root / \"AIM460_release_bundle.zip\"\nif bundle_dir.exists(): shutil.rmtree(bundle_dir)\nbundle_dir.mkdir(parents=True)\n\n# copy newer notebook + exports\nshutil.copy2(NEW_NB, bundle_dir / Path(NEW_NB).name)\nshutil.copytree(exports, bundle_dir / \"exports\")\n\n# environment freeze + requirements\nenv_file = bundle_dir / \"environment_freeze.txt\"\ntry:\n    reqs = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n    env_file.write_text(reqs)\nexcept Exception as e:\n    env_file.write_text(f\"Could not pip freeze: {e}\\n\")\n\nreqs_path = bundle_dir / \"requirements.txt\"\nlines = env_file.read_text().splitlines()\nkeep_prefixes = (\"numpy\",\"pandas\",\"scikit-learn\",\"matplotlib\",\"torch\",\"transformers\",\"datasets\",\"tqdm\",\"fairlearn\")\npicked = [ln for ln in lines if ln.lower().startswith(keep_prefixes)]\nreqs_path.write_text(\"\\n\".join(picked or lines))\n\n(bundle_dir / \"README.md\").write_text(\n\"\"\"# AIM460 Cross-Domain Algorithmic Fairness Benchmark\nContents:\n- Final notebook\n- exports/ (tradeoff PNGs + CSV summaries)\n- environment_freeze.txt, requirements.txt\n\nRe-run: `pip install -r requirements.txt`, open notebook, Run All.\n\"\"\"\n)\n\nif bundle_zip.exists(): bundle_zip.unlink()\nshutil.make_archive(str(bundle_zip.with_suffix(\"\")), \"zip\", bundle_dir)\n\n# ---------- 2) Overleaf pack ----------\noverleaf_dir = root / \"overleaf_pack\"\noverleaf_zip = root / \"overleaf_pack.zip\"\nif overleaf_dir.exists(): shutil.rmtree(overleaf_dir)\nfigdir = overleaf_dir / \"figures\"\nfigdir.mkdir(parents=True)\n\n# copy canonical figures + any *tradeoff*.png\nwanted = [\"income_tradeoff.png\",\"employment_tradeoff.png\",\"compas_tradeoff.png\",\"civilcomments_tradeoff.png\"]\npresent = set()\nfor name in wanted:\n    p = exports / name\n    if p.exists(): shutil.copy2(p, figdir / name); present.add(name)\nfor p in exports.glob(\"*tradeoff*.png\"):\n    dst = figdir / p.name\n    if not dst.exists(): shutil.copy2(p, dst); present.add(p.name)\n\n# copy results summary csv if present\nsummary_csv = exports / \"results_summary_all.csv\"\nhas_csv = summary_csv.exists()\nif has_csv:\n    shutil.copy2(summary_csv, overleaf_dir / \"results_summary_all.csv\")\n\n# skeleton main.tex with placeholders\nmain_tex = r\"\"\"\n\\documentclass[conference]{IEEEtran}\n\\IEEEoverridecommandlockouts\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage{amsmath}\n\\usepackage{hyperref}\n\\usepackage{siunitx}\n\\usepackage{xcolor}\n\n\\title{Cross-Domain Algorithmic Fairness Benchmark}\n\\author{\\IEEEauthorblockN{Your Name}\n\\IEEEauthorblockA{AIM460 Capstone \\\\\\texttt{youremail@example.com}}}\n\\begin{document}\\maketitle\n\n\\begin{abstract}\nWe present a reproducible benchmark for algorithmic fairness across multiple datasets and modalities.\n\\end{abstract}\n\n\\section{Introduction}\nBrief overview...\n\n\\section{Related Work}\nFoundational literature...\n\n\\section{Methods}\n% <<< INJECT: METHODS_FROM_OLD_NOTEBOOK >>>\n\n\\section{Datasets}\nWe include COMPAS, CivilComments, and Folktables tasks (income/employment)...\n\n\\section{Experimental Setup}\nUnified splits; Fairlearn metrics; mitigation strategies...\n\n\\section{Results}\n% <<< INJECT: RESULTS_TABLES >>>\n\n\\begin{figure}[!t]\\centering\n\\includegraphics[width=\\linewidth]{figures/income_tradeoff.png}\n\\caption{Income: accuracy--fairness trade-off.}\n\\end{figure}\n\n\\begin{figure}[!t]\\centering\n\\includegraphics[width=\\linewidth]{figures/employment_tradeoff.png}\n\\caption{Employment: accuracy--fairness trade-off.}\n\\end{figure}\n\n\\begin{figure}[!t]\\centering\n\\includegraphics[width=\\linewidth]{figures/compas_tradeoff.png}\n\\caption{COMPAS: accuracy--fairness trade-off.}\n\\end{figure}\n\n\\begin{figure}[!t]\\centering\n\\includegraphics[width=\\linewidth]{figures/civilcomments_tradeoff.png}\n\\caption{CivilComments: accuracy--fairness trade-off.}\n\\end{figure}\n\n\\section{Discussion}\nObserved tensions and generalization notes...\n\n\\section{Limitations}\n% <<< INJECT: LIMITATIONS_FROM_OLD_NOTEBOOK >>>\n\n\\section{Conclusion}\nRelease bundle and teaching utilityâ€¦\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{references}\n\\end{document}\n\"\"\".strip()\n(overleaf_dir / \"main.tex\").write_text(main_tex)\n\n(overleaf_dir / \"references.bib\").write_text(r\"\"\"\n@inproceedings{hardt2016equality,\n  title={Equality of Opportunity in Supervised Learning},\n  author={Hardt, Moritz and Price, Eric and Srebro, Nati},\n  booktitle={NeurIPS}, year={2016}\n}\n@book{barocas-hardt-narayanan,\n  title={Fairness and Machine Learning}, author={Barocas, Hardt, Narayanan}, year={2019},\n  note={\\url{https://fairmlbook.org}}\n}\n@article{mehrabi2019survey,\n  title={A Survey on Bias and Fairness in Machine Learning},\n  author={Mehrabi et al.}, journal={arXiv:1908.09635}, year={2019}\n}\n\"\"\".strip())\n\n# ---------- 3) Lift Methods & Limitations from OLD notebook ----------\ndef load_md_cells(ipynb_path):\n    obj = json.loads(Path(ipynb_path).read_text())\n    return [\"\".join(c.get(\"source\",\"\")) for c in obj.get(\"cells\",[]) if c.get(\"cell_type\")==\"markdown\"]\n\nmd_cells = load_md_cells(OLD_NB)\n\nmethod_keys = [\n    \"pre-processing\",\"post-processing\",\"in-processing\",\n    \"Equalized Odds\",\"Equalized Opportunity\",\"ThresholdOptimizer\",\n    \"reweigh\",\"reweight\",\"DSAP\",\"reduction\",\"mitigation\"\n]\nlimit_keys = [\"limitation\",\"caveat\",\"bias\",\"label noise\",\"selection bias\",\"calibration\"]\n\ndef pick_blocks(cells, keys, k=6):\n    out, seen = [], set()\n    for c in cells:\n        if any(k_.lower() in c.lower() for k_ in keys):\n            c = c.strip()\n            if c and c not in seen:\n                out.append(c); seen.add(c)\n    return out[:k]\n\nmethods_blocks = pick_blocks(md_cells, method_keys)\nlimits_blocks = pick_blocks(md_cells, limit_keys)\n\nimport re\ndef md_to_tex(md):\n    md = md.replace(\"**\", r\"\\textbf{\").replace(\"__\", r\"\\textbf{\")\n    md = re.sub(r\"\\*(.+?)\\*\", r\"\\\\emph{\\1}\", md)\n    md = re.sub(r\"^[-*]\\s+\", r\"\\\\item \", md, flags=re.M)\n    if \"\\\\item \" in md and \"\\\\begin{itemize}\" not in md:\n        md = \"\\\\begin{itemize}\\n\" + md + \"\\n\\\\end{itemize}\"\n    return md\n\nmethods_tex = \"\\n\\n\".join(md_to_tex(b) for b in methods_blocks) or \\\n              \"We apply pre-, in-, and post-processing mitigations including reweighting, reduction-based Equalized Odds, and threshold optimization.\"\nlimits_tex  = \"\\n\\n\".join(md_to_tex(b) for b in limits_blocks) or \\\n              \"We focus on group metrics; dataset artifacts (e.g., label noise, selection bias) and calibration are not fully addressed.\"\n\nmpath = overleaf_dir / \"main.tex\"\nm = mpath.read_text()\nm = m.replace(\"% <<< INJECT: METHODS_FROM_OLD_NOTEBOOK >>>\", methods_tex)\nm = m.replace(\"% <<< INJECT: LIMITATIONS_FROM_OLD_NOTEBOOK >>>\", limits_tex)\nmpath.write_text(m)\n\n# also save notes file\ndocs_dir = root / \"docs\"; docs_dir.mkdir(exist_ok=True)\n(docs_dir / \"methods_notes.md\").write_text(\"# Methods Notes (lifted from older notebook)\\n\\n\" + \"\\n\\n---\\n\\n\".join(methods_blocks or [\"(No extracted blocksâ€”placeholder)\"]))\n\n# ---------- 4) Auto-build LaTeX results tables from existing CSV ----------\nif has_csv:\n    df = pd.read_csv(summary_csv)\n    def norm(s): return s.strip().lower().replace(\" \",\"_\")\n    df.columns = [norm(c) for c in df.columns]\n    gcols = [c for c in [\"dataset\",\"task\",\"domain\",\"corpus\",\"benchmark\"] if c in df.columns]\n    assert gcols, \"Couldn't find a dataset-like column (need 'dataset' or 'task' in CSV).\"\n    gcol = gcols[0]\n    mcols_all = [c for c in df.columns if c not in {gcol,\"model\",\"clf\",\"estimator\",\"mitigation\",\"strategy\",\"method\",\"split\",\"seed\",\"run\",\"notes\"} and np.issubdtype(df[c].dtype, np.number)]\n    preferred = [\"accuracy\",\"auroc\",\"f1\",\"demographic_parity_difference\",\"equalized_odds_tpr_diff\",\"equalized_odds_fpr_diff\"]\n    metrics = [m for m in preferred if m in mcols_all] or mcols_all\n    pretty = {\n        \"accuracy\":\"Accuracy\",\"auroc\":\"AUROC\",\"f1\":\"F1\",\"macro_f1\":\"Macro-F1\",\n        \"demographic_parity_difference\":\"DPD (|Î”|)\",\"equalized_odds_tpr_diff\":\"EO TPR Î” (|Î”|)\",\"equalized_odds_fpr_diff\":\"EO FPR Î” (|Î”|)\"\n    }\n    fairness_like = set([\"demographic_parity_difference\",\"equalized_odds_tpr_diff\",\"equalized_odds_fpr_diff\",\"calibration_error\"])\n    if \"split\" in df.columns:\n        test_mask = df[\"split\"].str.lower().eq(\"test\")\n        if test_mask.any(): df = df[test_mask].copy()\n    if \"model\" not in df.columns: df[\"model\"] = df.get(\"clf\", df.get(\"estimator\",\"(model)\"))\n    if \"mitigation\" not in df.columns: df[\"mitigation\"] = df.get(\"strategy\", df.get(\"method\",\"(none)\"))\n    def fmt(x):\n        if pd.isna(x): return \"\"\n        return f\"{x:.3f}\" if abs(x) < 10 else f\"{x:.2f}\"\n    def best_mask(s, fairness=False):\n        if fairness: s_ = s.abs(); b = s_.min()\n        else: b = s.max()\n        return s.eq(b)\n    tables=[]\n    for ds, sub in df.groupby(gcol):\n        cols = [\"model\",\"mitigation\"] + [m for m in metrics if m in sub.columns]\n        t = sub[cols].reset_index(drop=True).copy()\n        # bold masks\n        masks={}\n        for mtr in metrics:\n            if mtr in t.columns:\n                masks[mtr]=best_mask(t[mtr], fairness=mtr in fairness_like)\n        lines=[]\n        for i,row in t.iterrows():\n            cells=[str(row[\"model\"]), str(row[\"mitigation\"])]\n            for mtr in metrics:\n                if mtr not in t.columns: continue\n                s=fmt(row[mtr])\n                if masks[mtr].iloc[i]: s=f\"\\\\textbf{{{s}}}\"\n                cells.append(s)\n            lines.append(\" & \".join(cells) + \" \\\\\\\\\")\n        headers=[\"Model\",\"Mitigation\"] + [pretty.get(m, m.replace(\"_\",\" \").title()) for m in metrics if m in t.columns]\n        colspec = \"ll\" + \"c\"*(len(headers)-2)\n        tab = []\n        tab += [\"\\\\begin{table}[!t]\",\"\\\\centering\",\n                f\"\\\\caption{{{ds}: summary metrics and fairness deltas (best per column in bold).}}\",\n                f\"\\\\label{{tab:{re.sub(r'[^a-zA-Z0-9]+','-', str(ds).lower())}}}\",\n                f\"\\\\\\\\begin{{tabular}}{{{colspec}}}\", \"\\\\toprule\",\n\n                \" & \".join(headers) + \" \\\\\\\\\",\"\\\\midrule\"]\n        tab += lines\n        tab += [\"\\\\bottomrule\",\"\\\\end{tabular}\",\"\\\\end{table}\"]\n        tables.append(\"\\n\\n\".join(tab))\n    (overleaf_dir / \"results_tables.tex\").write_text(\"\\n\\n\".join(tables))\n    # inject \\input\n    mt = (overleaf_dir / \"main.tex\").read_text()\n    ph = \"% <<< INJECT: RESULTS_TABLES >>>\"\n    snippet = \"\\n\\n% Auto-generated results tables\\n\\\\input{results_tables}\\n\"\n    if ph in mt:\n        mt = mt.replace(ph, snippet)\n    else:\n        mt = mt.replace(\"\\\\section{Discussion}\", snippet + \"\\n\\\\section{Discussion}\")\n    (overleaf_dir / \"main.tex\").write_text(mt)\n\n# zip Overleaf pack\nif overleaf_zip.exists(): overleaf_zip.unlink()\nshutil.make_archive(str(overleaf_zip.with_suffix(\"\")), \"zip\", overleaf_dir)\n\nprint(\"âœ… Done.\")\nprint(\"Release bundle:\", bundle_zip)\nprint(\"Overleaf pack:\", overleaf_zip)\nprint(\"Figures included:\", sorted(present))\nprint(\"Has results_summary_all.csv:\", has_csv)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03wmRZqz-vBC",
    "outputId": "12d7c6f9-c3d7-47f8-c88b-56407b840deb"
   },
   "id": "03wmRZqz-vBC",
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Done.\n",
      "Release bundle: /content/AIM460_release_bundle.zip\n",
      "Overleaf pack: /content/overleaf_pack.zip\n",
      "Figures included: ['civilcomments_tradeoff.png', 'compas_tradeoff.png', 'employment_tradeoff.png', 'fairface_dear_tradeoff.png', 'income_tradeoff.png']\n",
      "Has results_summary_all.csv: True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Fix GitHub render error: remove bad metadata.widgets and (optionally) clear outputs ---\n",
    "import nbformat, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "src = Path(\"/content/Maurrasse_AIM460_Fairness_Benchmark (1).ipynb\")  # your current notebook\n",
    "dst = Path(\"/content/Maurrasse_AIM460_Fairness_Benchmark_github.ipynb\")  # clean copy for GitHub\n",
    "\n",
    "nb = nbformat.read(src, as_version=4)\n",
    "\n",
    "# If a widgets block exists but is malformed, drop it entirely (GitHub doesnâ€™t need it to render).\n",
    "widgets = nb.metadata.get(\"widgets\", None)\n",
    "if widgets is not None:\n",
    "    # safest: remove whole block\n",
    "    nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "# (Optional) also clear heavy outputs to keep file small and render fast.\n",
    "for cell in nb.cells:\n",
    "    if cell.get(\"cell_type\") == \"code\":\n",
    "        cell[\"outputs\"] = []\n",
    "        cell[\"execution_count\"] = None\n",
    "\n",
    "nbformat.write(nb, dst)\n",
    "print(\"âœ… Wrote cleaned notebook:\", dst)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoBYxiLw-0qz",
    "outputId": "1e3a3db6-9b6f-4203-ebe3-26360da432a9"
   },
   "id": "QoBYxiLw-0qz",
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Wrote cleaned notebook: /content/Maurrasse_AIM460_Fairness_Benchmark_github.ipynb\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# PURPOSE: Initialize environment and imports\n# - Set random seeds (numpy/torch/sklearn)\n# - Import pandas, numpy, matplotlib, sklearn, fairlearn, transformers\n# - Configure warnings/logging for clean output\n\n# === Cross-domain fairnessâ€“utility tradeoff summary plot ===\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Path to your consolidated results CSV (should already exist)\ncsv_path = \"/content/exports/results_summary_all.csv\"\n\ndf = pd.read_csv(csv_path)\nassert {\"dataset\",\"method\",\"accuracy\",\"eqodds_tpr_gap\"}.issubset(df.columns), \"CSV missing key columns.\"\n\n# Clean up dataset and method names for display\ndisplay_names = {\n    \"acs_income\": \"Income\",\n    \"acs_employment\": \"Employment\",\n    \"compas\": \"COMPAS\",\n    \"civilcomments\": \"CivilComments\",\n    \"fairface\": \"FairFace\"\n}\ndf[\"dataset_label\"] = df[\"dataset\"].map(display_names).fillna(df[\"dataset\"])\ndf[\"method_label\"] = df[\"method\"].str.replace(\"_\", \" \").str.title()\n\n# Build the plot\nplt.figure(figsize=(7,5))\ncolors = {\n    \"Income\":\"#1f77b4\",\n    \"Employment\":\"#2ca02c\",\n    \"COMPAS\":\"#d62728\",\n    \"CivilComments\":\"#9467bd\",\n    \"FairFace\":\"#ff7f0e\"\n}\n\nfor dset, sub in df.groupby(\"dataset_label\"):\n    plt.scatter(\n        sub[\"eqodds_tpr_gap\"],\n        sub[\"accuracy\"],\n        label=dset,\n        s=80,\n        alpha=0.8,\n        color=colors.get(dset,\"gray\"),\n        edgecolor=\"black\"\n    )\n    # annotate best fairness per dataset\n    best_idx = sub[\"eqodds_tpr_gap\"].idxmin()\n    if pd.notna(best_idx):\n        x, y = sub.loc[best_idx, [\"eqodds_tpr_gap\",\"accuracy\"]]\n        plt.text(x, y+0.005, sub.loc[best_idx,\"method_label\"], fontsize=8, ha=\"center\")\n\nplt.xlabel(\"Equalized Odds TPR Gap (â†“ Fairer)\")\nplt.ylabel(\"Accuracy (â†‘ Better)\")\nplt.title(\"Cross-Domain Fairnessâ€“Utility Trade-off\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.legend(title=\"Dataset\", loc=\"best\")\nplt.tight_layout()\n\n# Save the plot\noutdir = Path(\"/content/exports\"); outdir.mkdir(parents=True, exist_ok=True)\nout_path = outdir / \"all_datasets_tradeoff.png\"\nplt.savefig(out_path, dpi=300)\nplt.close()\nprint(f\"âœ… Saved summary plot to {out_path}\")\n",
   "metadata": {
    "id": "qkVhabmwf3Ra"
   },
   "id": "qkVhabmwf3Ra",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2779fb07",
   "metadata": {
    "id": "2779fb07"
   },
   "source": [
    "### âœ… Final Export Paths\n",
    "- `/content/exports/income_tradeoff.png`\n",
    "- `/content/exports/employment_tradeoff.png`\n",
    "- `/content/exports/compas_tradeoff.png` (if run)\n",
    "- `/content/exports/civilcomments_tradeoff.png` (if run)\n",
    "- `/content/exports/fairface_dear_tradeoff.png` (if run)\n",
    "- `/content/exports/results_summary_all.csv`\n",
    "- Sanitized: `/content/AIM460_Fairness_Benchmark_Final_clean.ipynb`\n",
    "- Release ZIP: `/content/AIM460_release_bundle.zip`\n",
    "- Overleaf ZIP: `/content/overleaf_pack.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Data Cleaning & Imputation (KNN etc.)\n\n*Placeholder: add code or confirm this step was done earlier in the notebook.*\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Feature Engineering\n\n*Placeholder: add code or confirm this step was done earlier in the notebook.*\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Metrics (Utility + Fairness)\n\n*Placeholder: add code or confirm this step was done earlier in the notebook.*\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) Fairness Mitigations\n\n*Placeholder: add code or confirm this step was done earlier in the notebook.*\n"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "88830714"
   ],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}